# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y0eBfRKpLER1BDWGGp1sBNve2Iofyei-
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install onnxruntime onnxruntime-tools

# main.py
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer
import onnxruntime
import numpy as np
import joblib

# ✅ 모델/토크나이저/라벨 인코더 로딩
tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")

# ONNX 양자화 모델 (경로는 배포 환경에 맞게 조정)
onnx_path = "subcat_model_quant.onnx"
session = onnxruntime.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])

# LabelEncoder 로딩 (경로는 배포 환경에 맞게 조정)
label_encoder = joblib.load("/content/drive/MyDrive/subcategory_label_encoder.pkl")

# ✅ FastAPI 앱 생성
app = FastAPI()

# ✅ 입력 데이터 형식 정의
class Question(BaseModel):
    text: str

# ✅ 예측 API
@app.post("/predict")
def predict(question: Question):
    # 전처리
    inputs = tokenizer(question.text, return_tensors="np", padding=True, truncation=True, max_length=512)

    # ONNX 추론
    outputs = session.run(None, {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"]
    })

    logits = outputs[0]
    pred_index = int(np.argmax(logits, axis=-1)[0])

    # 라벨 디코딩
    pred_label = label_encoder.inverse_transform([pred_index])[0]

    return {
        "input": question.text,
        "predicted_index": pred_index,
        "predicted_label": pred_label
    }